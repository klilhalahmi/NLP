{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLHeV9icIk-b"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# Natural Language Processing\n",
        "\n",
        "## Assignment 002 - POS Tagging with Universal Dependencies\n",
        "\n",
        "> Notebook by:\n",
        "> - NLP Course Stuff\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        | Content / Changes                                                   |\n",
        "|---------|------------|-------------|---------------------------------------------------------------------|\n",
        "| 0.2.000 | 15/05/2024 | course staff| Updated `forward` method (Part 3) documentation to clarify return values as log probabilities. |\n",
        "| 0.1.000 | 01/05/2024 | course staff| First version                                                       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCFoQxO0H0yk"
      },
      "source": [
        "## Overview\n",
        "In this assignment, you will train and evaluate a Part-of-Speech (POS) tagger using data from the Universal Dependencies (UD) project. POS taggers assign parts of speech to each word in a sentence, such as noun, verb, adjective, etc., which are crucial for many natural language processing tasks.\n",
        "\n",
        "## Dataset\n",
        "Utilize the English Web Treebank from the Universal Dependencies project. You can access and explore the dataset [here](https://universaldependencies.org/). For a better understanding of the project and the data format, visit the [introduction page](https://universaldependencies.org/introduction.html).\n",
        "\n",
        "## Tasks Overview\n",
        "\n",
        "#### Part 1: Dataset Preparation\n",
        "- **Objective**: Get the Universal Dependencies dataset ready for the tagging tasks.\n",
        "- **Activities**: Download, preprocess, and format the dataset.\n",
        "\n",
        "#### Part 2: HMM Tagger\n",
        "- **Objective**: Create and assess a POS tagger using the Hidden Markov Model.\n",
        "- **Activities**: Construct, train, and evaluate the HMM tagger.\n",
        "\n",
        "#### Part 3: Feed-Forward Neural Network Tagger\n",
        "- **Objective**: Build a POS tagger using a feed-forward neural network with word embeddings.\n",
        "- **Activities**: Develop and train the model in PyTorch, then evaluate its effectiveness.\n",
        "\n",
        "#### Part 4: NLTK MEMM Tagger\n",
        "- **Objective**: Implement and test a MEMM-based POS tagger using NLTK.\n",
        "- **Activities**: Train the MEMM tagger, then evaluate its performance.\n",
        "\n",
        "#### Part 5: Models' Comparison\n",
        "- **Objective**: Evaluate and contrast the performance of different models.\n",
        "- **Activities**: Address two open-ended questions.\n",
        "\n",
        "\n",
        "## Your Implementation\n",
        "\n",
        "Please create a local copy of this template Colab's Notebook:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FfXDvRMALIsd-IzPdf_Fn92OLVHjSY9I#scrollTo=JCFoQxO0H0yk)\n",
        "\n",
        "The assignment's instructions are there; follow the notebook.\n",
        "\n",
        "## Submission\n",
        "- **Notebook Link**: Add the URL to your assignment's notebook in the `notebook_link.txt` file, following the format provided in the example.\n",
        "- **Access**: Ensure the link has edit permissions enabled to allow modifications if needed.\n",
        "- **Deadline**: <font color='green'>21/05/2024</font>.\n",
        "- **Platform**: Continue using GitHub for submissions. Push your project to the team repository and monitor the test results under the actions section.\n",
        "\n",
        "Good Luck ðŸ¤—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iWKz8IfKi5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569a4b7f-b91a-48a9-b016-b945270b6c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for conllutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Prerequisite: Install the conllutils library before proceeding with the tasks\n",
        "!pip install --q conllutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRm7zcfq56HF"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import random\n",
        "import operator\n",
        "import json\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "# Data Handling and Numerical Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from google.colab import files\n",
        "\n",
        "# Machine Learning and Evaluation Libraries\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# Natural Language Processing Libraries\n",
        "import nltk\n",
        "from nltk.tag import tnt\n",
        "from nltk.metrics import ConfusionMatrix, precision, recall, f_measure\n",
        "\n",
        "# Deep Learning Libraries (PyTorch)\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# CoNLL Utilities for Data Handling\n",
        "import conllutils\n",
        "import gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-Xvqip6Teu"
      },
      "source": [
        "# Part 1 - Dataset Preparation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTv6rMt0oIw9"
      },
      "source": [
        "For each package you use, set the random seed to 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtM4HY2LoYDn"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "# Set the random seed for Python\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set the random seed for numpy\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Set the random seed for pytorch\n",
        "th.manual_seed(SEED)\n",
        "\n",
        "# If using CUDA (for GPU operations)\n",
        "th.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuvbl0hooXXx"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "### Step 1: Access the Dataset\n",
        "The GUM dataset, which is part of the English corpora under Universal Dependencies, is specifically curated for academic and research purposes. You can download the dataset directly from the following GitHub repository:\n",
        "\n",
        "[UD English-GUM](https://github.com/UniversalDependencies/UD_English-GUM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsZsyTVC6Sw0",
        "outputId": "83bde94b-ff2a-4275-a97c-588e32943917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UD_English-GUM'...\n",
            "remote: Enumerating objects: 6573, done.\u001b[K\n",
            "remote: Counting objects: 100% (1600/1600), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 6573 (delta 1484), reused 1394 (delta 1284), pack-reused 4973\u001b[K\n",
            "Receiving objects: 100% (6573/6573), 59.82 MiB | 19.03 MiB/s, done.\n",
            "Resolving deltas: 100% (6118/6118), done.\n",
            "Updating files: 100% (230/230), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_English-GUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "airorSOzSRm_"
      },
      "source": [
        "### Step 2: Reading the Data\n",
        "We will use the (train/dev/test) files:\n",
        "\n",
        "\n",
        "```\n",
        "UD_English-GUM/en_gum-ud-train.conllu\n",
        "UD_English-GUM/en_gum-ud-dev.conllu\n",
        "UD_English-GUM/en_gum-ud-test.conllu\n",
        "```\n",
        "\n",
        "## CoNLL-U Format\n",
        "They are all formatted in the CoNLL-U format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n",
        "\n",
        "## Task: Create a Read Data Function\n",
        "\n",
        "### Function Specification\n",
        "Create a function named `read_data` that:\n",
        "- Takes a file path to a `.conllu` file as input.\n",
        "- Returns a list of lists, where each inner list represents a sentence.\n",
        "- Each sentence is composed of tuples containing the word ('form') and its corresponding Universal POS tag ('upos').\n",
        "\n",
        "The word is located in the column named 'form' and the POS tag in the column named 'upos' of the CoNLL-U format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkepUJYENXPq"
      },
      "outputs": [],
      "source": [
        "DataType = list[list[tuple[str, str]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYt7poZ3V2LV"
      },
      "outputs": [],
      "source": [
        "def read_data(filepath: str) -> DataType:\n",
        "    \"\"\"\n",
        "    Reads a CoNLL-U formatted file and extracts sentences as lists of (word, POS tag) tuples.\n",
        "\n",
        "    Args:\n",
        "    filepath (str): The path to the .conllu file to be read.\n",
        "\n",
        "    Returns:\n",
        "    List[List[Tuple[str, str]]]: A list of sentences, where each sentence is a list of tuples containing the word and its POS tag.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    for sentence in conllutils.read_conllu(filepath):\n",
        "        output.append([(word[\"form\"], word[\"upos\"]) for word in sentence if 'upos' in word])\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmRpi7gXknQC"
      },
      "outputs": [],
      "source": [
        "# Run this block once `read_data` is implemented.\n",
        "train_dataset = read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")\n",
        "dev_dataset = read_data(\"UD_English-GUM/en_gum-ud-dev.conllu\")\n",
        "test_dataset = read_data(\"UD_English-GUM/en_gum-ud-test.conllu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2E18JUi2YeD"
      },
      "source": [
        "## Task: Create a Vocabulary Generation Function\n",
        "\n",
        "### Function Specification\n",
        "Create a function named `generate_vocabs` for mapping words and tags into unique numbers so that we can use them in the tagging algorithms you will implement below. The function:\n",
        "- Takes a list of sentences as input, where each sentence is composed of tuples containing a word and its corresponding Universal POS tag.\n",
        "- Returns a tuple of two dictionaries:\n",
        "  - The first dictionary maps each unique word to a unique integer.\n",
        "  - The second dictionary maps each unique POS tag to a unique integer.\n",
        "\n",
        "Each word and POS tag should be mapped starting from 0, with each new word or tag encountered receiving the next sequential integer. This function is essential for converting textual data into a numerical format that can be used by tagging algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j47ImFVyyoO4"
      },
      "outputs": [],
      "source": [
        "def generate_vocabs(datasets: list[DataType]) -> tuple[dict[str, int], dict[str, int]]:\n",
        "  \"\"\"\n",
        "  Generates vocabularies mapping words and tags to unique indices from a list of datasets.\n",
        "\n",
        "  Args:\n",
        "  datasets (list): A list of datasets where each dataset contains sentences formatted as lists of (word, tag) tuples.\n",
        "\n",
        "  Returns:\n",
        "  Tuple[dict[str, int], dict[str, int]]: A tuple of two dictionaries:\n",
        "      - The first dictionary maps each unique word to a unique integer.\n",
        "      - The second dictionary maps each unique POS tag to a unique integer.\n",
        "      Each dictionary includes a special '<<UNK>>' entry mapped to 0 to handle unknown words or tags.\n",
        "  \"\"\"\n",
        "  words_vocab = {\"<<UNK>>\": 0}\n",
        "  tags_vocab = {\"<<UNK>>\": 0}\n",
        "  word_index = 1\n",
        "  tag_index = 1\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  for dataset in datasets:\n",
        "    for sentence in dataset:\n",
        "      for word, tag in sentence:\n",
        "        if word not in words_vocab:\n",
        "          words_vocab[word] = word_index\n",
        "          word_index += 1\n",
        "        if tag not in tags_vocab:\n",
        "          tags_vocab[tag] = tag_index\n",
        "          tag_index += 1\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return words_vocab, tags_vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vumzm2lB7p1-"
      },
      "outputs": [],
      "source": [
        "def train_evaluate_tagger(tagger, train_dataset: DataType=train_dataset, eval_dataset: DataType=test_dataset, train=True):\n",
        "    \"\"\"\n",
        "    Trains (optional) and evaluates a POS tagger, returning performance metrics.\n",
        "\n",
        "    This function trains the given tagger if specified, and evaluates it on a provided dataset.\n",
        "    It calculates and returns the macro-average precision, recall, and F1-score.\n",
        "\n",
        "    Args:\n",
        "        tagger (HMMTagger or EmbeddingsTagger): The POS tagger to be trained and evaluated.\n",
        "        train_dataset (List[List[Tuple[str, str]]]): The dataset to train the tagger.\n",
        "        eval_dataset (List[List[Tuple[str, str]]]): The dataset to evaluate the tagger.\n",
        "        train (bool): A flag indicating whether the tagger should be trained.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing average precision, recall, and F1-score.\n",
        "\n",
        "    The function also prints a classification report for detailed performance analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    true_tags = []\n",
        "    predicted_tags = []\n",
        "\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    if train:\n",
        "        tagger.fit(train_dataset)\n",
        "    for sentence in eval_dataset:\n",
        "        words = [word for word, _ in sentence]\n",
        "        true_tags.extend([tag for _, tag in sentence])\n",
        "        predicted_tags.extend([tag for _, tag in tagger.inference(words)])\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "    # Compute precision, recall, F1-score, and support for each class\n",
        "    precision, recall, f1_score, support = precision_recall_fscore_support(true_tags, predicted_tags, average=None)\n",
        "\n",
        "    # Display classification report\n",
        "    report = classification_report(true_tags, predicted_tags)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Return the macro-average metrics\n",
        "    avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(true_tags, predicted_tags, average='macro')\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwzutNN21Z6m"
      },
      "outputs": [],
      "source": [
        "# Run this block once `generate_vocabs` is implemented.\n",
        "words_vocab, tags_vocab = generate_vocabs([train_dataset, dev_dataset, test_dataset])\n",
        "reversed_words_vocab = {v: k for k, v in words_vocab.items()}\n",
        "reversed_tags_vocab = {v: k for k, v in tags_vocab.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK9iZIq8i0X"
      },
      "source": [
        "# Part 2 - HMM Tagger\n",
        "\n",
        "### Task Description\n",
        "Implement a class `HMMTagger` to perform POS tagging using a Hidden Markov Model (HMM).\n",
        "\n",
        "### Class Methods\n",
        "- `fit`: This method should compute the transition probabilities matrix (A), emission probabilities matrix (B), and initial state probabilities vector (Pi) based on the training data. These matrices should reflect probabilities of transitions between tags, emissions of words given tags, and initial tag probabilities, respectively.\n",
        "- `inference`: Implement this method to predict the best tag sequence for a given input sentence using the Viterbi decoding algorithm. The Viterbi algorithm is provided below.\n",
        "### Additional Guidance\n",
        "1. **Use of Vocabularies**: Utilize the vocabularies generated in Part 1. These should include a special entry for unknown words and tags (`<<UNK>>` at index 0). The indices in your vocabularies will correspond to the rows and columns of your A, B, and Pi matrices (np.array).\n",
        "2. **Smoothing**: Apply Add-One Smoothing to all probability calculations to avoid zero probabilities. This technique adjusts the frequency counts for each observed event by adding one to each count.\n",
        "3. **Word Conversion**: During inference, convert each word of the input sentence into its corresponding index using the word vocabulary. If a word is not found, use the index for `<<UNK>>`.\n",
        "\n",
        "### Implementation Tips\n",
        "- You can use the vocab dictionaries directly, no need to pass them as a parameter to the functions.\n",
        "- You may add functions to `HMMTagger` as needed.\n",
        "- Ensure that your A, B, and Pi matrices handle unseen words/tags gracefully using the `<<UNK>>` index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpH7GuiQ9L6W"
      },
      "outputs": [],
      "source": [
        "class HMMTagger:\n",
        "  def __init__(self):\n",
        "      \"\"\"\n",
        "      Initializes the HMMTagger class with necessary attributes.\n",
        "      \"\"\"\n",
        "      self._Ï€ = np.ones(len(tags_vocab))  # Initial state probabilities\n",
        "      self._A = np.ones((len(tags_vocab), len(tags_vocab)))  # Transition probabilities\n",
        "      self._B = np.ones((len(tags_vocab), len(words_vocab)))  # Emission probabilities\n",
        "\n",
        "  def fit(self, dataset: DataType):\n",
        "      \"\"\"\n",
        "      Trains the HMM model on the provided dataset.\n",
        "\n",
        "      Args:\n",
        "          dataset (list): The training dataset containing sentences as lists of (word, tag) tuples.\n",
        "      \"\"\"\n",
        "      # TO DO ----------------------------------------------------------------------\n",
        "      self._Ï€[0] =1\n",
        "      for sentence in dataset:\n",
        "          for i, (word, tag) in enumerate(sentence):\n",
        "              if i == 0:\n",
        "                  self._Ï€[tags_vocab.get(tag, 0)] += 1\n",
        "                  self._A[0, tags_vocab.get(tag, 0)] += 1\n",
        "              else:\n",
        "                  self._Ï€[tags_vocab.get(tag, 0)] += 1\n",
        "                  self._A[tags_vocab[sentence[i - 1][1]], tags_vocab.get(tag, 0)] += 1\n",
        "\n",
        "              self._B[tags_vocab.get(tag, 0), words_vocab.get(word, 0)] += 1\n",
        "      self._Ï€ /= np.sum(self._Ï€)\n",
        "      self._A /= np.sum(self._A, axis=1, keepdims=True)\n",
        "      self._B /=  np.sum(self._B, axis=1, keepdims=True)\n",
        "\n",
        "      # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "  def inference(self, sentence: list) -> list[Tuple[str, str]]:\n",
        "      \"\"\"\n",
        "      Predicts the best tag sequence for a given input sentence using the Viterbi decoding algorithm.\n",
        "\n",
        "      Args:\n",
        "          sentence (list): The sentence to tag, as a list of words.\n",
        "\n",
        "      Returns:\n",
        "          List[Tuple[str, str]]: Each word in the input sentence paired with its predicted tag.\n",
        "      \"\"\"\n",
        "      # TO DO ----------------------------------------------------------------------\n",
        "      word_list = [words_vocab.get(word, 0) for word in sentence]\n",
        "      tag_list = viterbi(word_list, self._A, self._B, self._Ï€)\n",
        "      return [(sentence[i], reversed_tags_vocab[tag]) for i, tag in enumerate(tag_list)]\n",
        "      # TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni-3uwVmXiKB"
      },
      "source": [
        "### Optional Task: Implement the Viterbi Algorithm\n",
        "Implement the `viterbi` function to perform POS tagging using the Viterbi decoding algorithm. This algorithm finds the most probable sequence of hidden states (POS tags in this case) given a sequence of observations (words in the sentence).\n",
        "\n",
        "### Instructions\n",
        "- **Pre-Implemented Code**: We provide a pre-implemented version of the Viterbi algorithm for your convenience. This implementation is fully functional and can be used directly in your HMM tagger.\n",
        "  \n",
        "- **Implementation Challenge**: Although a pre-implemented version is available, we encourage you to implement the Viterbi algorithm yourself. Doing so will help you understand the dynamics of dynamic programming in the context of POS tagging. Follow the pseudocode provided in the lecture slides to develop your own version of the algorithm.\n",
        "\n",
        "### Steps for Implementation\n",
        "1. **Understand the Pseudocode**: Review the pseudocode provided in the slides from the class. Ensure you understand each step of the algorithm, including how the probabilities are updated and the backtracking process to recover the state sequence.\n",
        "  \n",
        "2. **Implement the Function**: Using the pseudocode as a guide, write your own `viterbi` function. Consider the matrices (A, B, and Pi) you prepared in the HMMTagger class as inputs along with the sequence of observations (word indices for a sentence).\n",
        "  \n",
        "3. **Test Your Implementation**: After implementing the function, test it with known inputs to ensure it produces the correct sequence of tags. Compare the results with those obtained from the pre-implemented version to validate your implementation.\n",
        "\n",
        "### Additional Tips\n",
        "- **Handle Edge Cases**: Consider edge cases such as very short sentences, sentences containing many unknown words, and varying sentence structures.\n",
        "- **Optimization**: Once your basic implementation is correct, think about potential optimizations to improve the efficiency of your code, especially if you are processing large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR6KJW2F9yqt"
      },
      "outputs": [],
      "source": [
        "def viterbi(word_list: list, A: np.ndarray, B: np.ndarray, Pi: np.ndarray):\n",
        "    \"\"\"\n",
        "    Executes the Viterbi algorithm to find the most likely state sequence given a sequence of observations.\n",
        "\n",
        "    The Viterbi algorithm is a dynamic programming algorithm used to compute the most likely sequence of hidden states\n",
        "    (called the Viterbi path) that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM).\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of indices corresponding to observed words.\n",
        "        A (numpy.ndarray): The state transition probability matrix of shape (num_states, num_states) where A[i][j] is the probability of transitioning from state i to state j.\n",
        "        B (numpy.ndarray): The emission probability matrix of shape (num_states, num_vocabulary) where B[i][j] is the probability of emitting symbol j from state i.\n",
        "        Pi (numpy.ndarray): The initial state probability vector of length num_states where Pi[i] is the probability of starting in state i.\n",
        "\n",
        "    Returns:\n",
        "        list: The most likely sequence of states (as indices) for the given sequence of observations.\n",
        "\n",
        "    The function uses three main steps: initialization, recursion, and termination:\n",
        "    - **Initialization**: Set the initial probabilities of being in each state.\n",
        "    - **Recursion**: For each subsequent observation, compute probabilities of each state based on the observation and transition probabilities from previous states.\n",
        "    - **Termination**: Backtrace to determine the most probable path through the state space.\n",
        "    \"\"\"\n",
        "    # Number of states\n",
        "    num_states = len(A)\n",
        "    # Length of the observed sequence\n",
        "    T = len(word_list)\n",
        "\n",
        "    # Create the path probability matrix V\n",
        "    V = [[0 for _ in range(num_states)] for _ in range(T)]\n",
        "    # Create a path backpointer matrix to store the argmax indices\n",
        "    path = [[0 for _ in range(num_states)] for _ in range(T)]\n",
        "\n",
        "    # Initialization step\n",
        "    for s in range(num_states):\n",
        "        V[0][s] = Pi[s] * B[s][word_list[0]]\n",
        "        path[0][s] = 0\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, T):\n",
        "        for s in range(num_states):\n",
        "            # Find the state with the maximum probability to reach state s\n",
        "            max_tr_prob = V[t-1][0] * A[0][s]\n",
        "            prev_state_selected = 0\n",
        "            for prev_state in range(1, num_states):\n",
        "                tr_prob = V[t-1][prev_state] * A[prev_state][s]\n",
        "                if tr_prob > max_tr_prob:\n",
        "                    max_tr_prob = tr_prob\n",
        "                    prev_state_selected = prev_state\n",
        "            # Multiply the max probability by the probability of observing the symbol at state s\n",
        "            max_prob = max_tr_prob * B[s][word_list[t]]\n",
        "            V[t][s] = max_prob\n",
        "            path[t][s] = prev_state_selected\n",
        "\n",
        "    # Termination step\n",
        "    # Find the best path by looking for the maximum probability in the last column\n",
        "    opt = []\n",
        "    max_prob = -1\n",
        "    best_last_state = 0\n",
        "    for s in range(num_states):\n",
        "        if V[T-1][s] > max_prob:\n",
        "            max_prob = V[T-1][s]\n",
        "            best_last_state = s\n",
        "    opt.append(best_last_state)\n",
        "\n",
        "    # Follow the back pointers to decode the best path\n",
        "    previous = best_last_state\n",
        "    for t in range(T-1, 0, -1):\n",
        "        opt.insert(0, path[t][previous])\n",
        "        previous = path[t][previous]\n",
        "\n",
        "    return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkj25vm2knij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0acd4634-9245-4fa2-fd91-8167040e386d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Example for using Viterbi algorithm (from the course slides)\n",
        "A = np.array([[0.3, 0.7], [0.2, 0.8]])\n",
        "B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])\n",
        "Pi = np.array([0.4, 0.6])\n",
        "\n",
        "viterbi([0, 3, 2, 0], A, B, Pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKUWMhyUgXy"
      },
      "source": [
        "## Train & Evaluate\n",
        "\n",
        "## Task: Implement the Train and Evaluate Tagger Function\n",
        "\n",
        "### Function Specification\n",
        "Create a function named `train_evaluate_tagger` that combines the training and evaluation processes for a tagging algorithm. This integrated function should:\n",
        "- **Input**:\n",
        "  - **tagger** (`HMMTagger` or `EmbeddingsTagger`): The POS tagger to be trained and evaluated.\n",
        "  - **train_dataset** (`List[List[Tuple[str, str]]]`): The dataset on which the tagger will be trained. Training is executed only if the `train` flag is set to True.\n",
        "  - **eval_dataset** (`List[List[Tuple[str, str]]]`): The dataset used for evaluating the tagger's performance.\n",
        "  - **train** (`bool`): A boolean flag indicating whether the training phase should be executed. If set to True, the `fit` method of the tagger will be called before evaluation.\n",
        "- **Functionality**:\n",
        "  - Train the tagger using the `train_dataset`.\n",
        "  - Evaluate the trained tagger on the `test_dataset` to compute performance metrics.\n",
        "  - Return the following evaluation metrics: accuracy, precision, recall, and F1-score.\n",
        "- **Outputs**: A tuple consisting of three elements: precision, recall, F1-score, as returned by the `precision_recall_fscore_support` function from the `sklearn.metrics` module, when called with the parameter `average=\"macro\"`.\n",
        "* Note the the support metric is missed (Why?).\n",
        "\n",
        "\n",
        "### Evaluation Metrics\n",
        "Upon execution, the `train_evaluate_tagger` function provides the following metrics:\n",
        "- **Macro-average Precision, Recall, and F1 Score**: These scores are calculated over all tags to assess the overall effectiveness of the tagger in recognizing the correct tags across different types of words.\n",
        "- **Performance Breakdown by Tag**: Detailed metrics for each tag, helping to identify which tags are most and least accurately predicted.\n",
        "- **Overall Word-Level Accuracy**: Measures the percentage of words correctly tagged by the tagger across the entire evaluation dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0uM2tozQmpq",
        "outputId": "3612e4cc-8285-42e0-e494-5ec0f89743cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     <<UNK>>       0.00      0.00      0.00         0\n",
            "         ADJ       0.79      0.75      0.77      1622\n",
            "         ADP       0.85      0.97      0.91      2481\n",
            "         ADV       0.85      0.74      0.79      1114\n",
            "         AUX       0.81      0.96      0.88      1189\n",
            "       CCONJ       0.97      0.97      0.97       839\n",
            "         DET       0.82      0.97      0.89      2111\n",
            "        INTJ       0.94      0.56      0.70       163\n",
            "        NOUN       0.81      0.83      0.82      4239\n",
            "         NUM       0.74      0.65      0.69       440\n",
            "        PART       0.86      0.79      0.82       519\n",
            "        PRON       0.84      0.97      0.90      1746\n",
            "       PROPN       0.84      0.47      0.60      1628\n",
            "       PUNCT       0.92      1.00      0.96      3027\n",
            "       SCONJ       0.87      0.53      0.66       340\n",
            "         SYM       0.47      0.26      0.33        35\n",
            "        VERB       0.88      0.76      0.81      2480\n",
            "           X       0.67      0.06      0.11        32\n",
            "\n",
            "    accuracy                           0.85     24005\n",
            "   macro avg       0.77      0.68      0.70     24005\n",
            "weighted avg       0.85      0.85      0.84     24005\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Train (optional) and Evaluate your HMMTagger here\n",
        "hmmTagger = HMMTagger()\n",
        "precision_hmm, recall_hmm, f1_hmm = train_evaluate_tagger(hmmTagger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3fPdEWBetDE"
      },
      "source": [
        "## Part 3 - POS Tagging with Pre-trained Word Embeddings\n",
        "\n",
        "### Task Overview\n",
        "Develop a feed-forward neural network for Part-of-Speech (POS) tagging using the PyTorch framework. This tagger leverages pre-trained word embeddings from the word2vec Google News dataset, enhancing the semantic understanding of words compared to traditional tagging methods.\n",
        "\n",
        "### Task 1: Initialization of Embeddings (`init_embeddings`)\n",
        "Create an `init_embeddings` method within the `EmbeddingsTagger` class to load and set up pre-trained word embeddings:\n",
        "- Load the Google News word vectors from a specified file path.\n",
        "- Initialize a matrix to hold these embeddings where each word in your vocabulary is represented by a vector. For words not in the pre-trained model, initialize their vectors randomly.\n",
        "-  <font color='red'>**NOTE:** Before you start implementing the init_embeddings method, make sure to create your own copy of the pre-trained embeddings (.bin file) in your Google Drive from [the following Google Drive folder](https://drive.google.com/drive/folders/1-HcSBfqaX0PCFiT8TsiYjFZRQJRm2R5v?usp=sharing). You will need to use the gensim library to load this file.</font>\n",
        "\n",
        "### Task 2: Implementation of `EmbeddingsTagger` Class\n",
        "Extend PyTorch's `nn.Module` to implement the `EmbeddingsTagger` class. This class should utilize the embeddings matrix and include:\n",
        "- An embedding layer that is initialized with the pre-trained embeddings.\n",
        "- A linear layer to combine embeddings of the current and previous words with a one-hot encoded previous tag to predict the current tag.\n",
        "- A `forward` method that outlines the data flow through the network.\n",
        "\n",
        "### Task 3: Model Training and Evaluation (`fit` and `inference` Methods)\n",
        "Implement training and inference functionalities within the `EmbeddingsTagger` class:\n",
        "- **Train Method (`fit`)**: Set up the model training using the specified training dataset. This includes iterating through the dataset, applying the model to predict tags, and updating model parameters based on the loss computation.\n",
        "- **Inference Method (`inference`)**: Configure the model to predict tags on a new dataset, assessing the model's effectiveness on unseen data.\n",
        "\n",
        "### Model Performance Monitoring\n",
        "Utilize the `train_evaluate_tagger` function to oversee the training process and evaluate the model:\n",
        "- Configure this function to handle model training with appropriate optimizer and loss settings.\n",
        "- Monitor and report the model's performance metrics during training and on a test dataset to ensure effective tagging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOGqtQtyUsA9"
      },
      "source": [
        "### Google News pre-trained embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XROdy8sHZ-Qu",
        "outputId": "963ab340-27bf-449a-f125-68922ee595a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljrof4K3Va70"
      },
      "outputs": [],
      "source": [
        "class EmbeddingsTagger(nn.Module):\n",
        "    def __init__(self, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Initializes an embeddings-based POS tagger that uses word embeddings from a pre-trained model.\n",
        "\n",
        "        Args:\n",
        "            device (str): The device (cpu or cuda) the model should operate on for tensor operations.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        super(EmbeddingsTagger, self).__init__()\n",
        "        self.embedding_dim = 300  # Dimension of Google News embeddings\n",
        "        self.tagset_size = len(tags_vocab)\n",
        "        self.word_embeddings = self.init_embeddings()\n",
        "        self.to(device)\n",
        "\n",
        "\n",
        "        # The input dimension to the linear layer is twice the embedding_dim (for two words)\n",
        "        # plus tagset_size (for the one-hot encoded previous tag)\n",
        "        self.linear_layer1 = nn.Linear(2 * self.embedding_dim + self.tagset_size, 128).to(self.device)\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n",
        "        self.linear_layer2 = nn.Linear(128, self.tagset_size).to(self.device)\n",
        "\n",
        "\n",
        "    def init_embeddings(self) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Loads word embeddings from a pre-trained Google News model and initializes an embedding layer.\n",
        "\n",
        "        Returns:\n",
        "            nn.Embedding: A PyTorch embedding layer with the pre-trained word embeddings.\n",
        "        \"\"\"\n",
        "        # Specify the path to the Google News model in your Google Drive\n",
        "        path_to_google_news_vectors = '/content/drive/My Drive/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "        # Load Google News Vectors\n",
        "        model = gensim.models.KeyedVectors.load_word2vec_format(path_to_google_news_vectors, binary=True)\n",
        "\n",
        "        # Prepare a matrix to hold the embeddings\n",
        "        embedding_matrix = np.zeros((len(words_vocab), 300))  # Ensure 'words_vocab' maps words to indices, 300 is the dimension of embeddings\n",
        "\n",
        "        # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "        for word, index in words_vocab.items():\n",
        "            if word in model:\n",
        "                embedding_matrix[index] = model[word]\n",
        "            else:\n",
        "                embedding_matrix[index] = np.random.rand(300)\n",
        "\n",
        "        embedding_tensor = th.tensor(embedding_matrix, dtype=th.float32).to(self.device)\n",
        "        embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        return embedding_layer\n",
        "\n",
        "        # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "    def forward(self, current_word_index: int, previous_word_index: int, prev_tag_one_hot: th.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the tagger to compute logits for each tag.\n",
        "\n",
        "        Args:\n",
        "            current_word_index (int): Index of the current word in the vocabulary.\n",
        "            previous_word_index (int): Index of the previous word in the vocabulary.\n",
        "            prev_tag_one_hot (th.tensor): One-hot encoded tensor of the previous tag.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Log probabilities for each tag.\n",
        "        \"\"\"\n",
        "        # TO DO ----------------------------------------------------------------------\n",
        "        current_word_index_tensor = th.tensor([current_word_index], dtype=th.long, device=self.device)\n",
        "        previous_word_index_tensor = th.tensor([previous_word_index], dtype=th.long, device=self.device)\n",
        "        current_word_embedding = self.word_embeddings(current_word_index_tensor)\n",
        "        previous_word_embedding = self.word_embeddings(previous_word_index_tensor)\n",
        "        prev_tag_one_hot = prev_tag_one_hot.to(self.device, dtype=th.float32)\n",
        "        current_word_embedding = current_word_embedding.to(dtype=th.float32)\n",
        "        previous_word_embedding = previous_word_embedding.to(dtype=th.float32)\n",
        "        x = th.cat((current_word_embedding.squeeze(), previous_word_embedding.squeeze(), prev_tag_one_hot), dim=0)\n",
        "        x = th.relu(self.linear_layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.linear_layer2(x)\n",
        "        logits = th.log_softmax(logits.unsqueeze(0), dim=-1)\n",
        "        return logits\n",
        "        # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def fit(self, dataset=list(train_dataset), epochs=10):\n",
        "        \"\"\"\n",
        "        Trains the tagger on the provided dataset for a specified number of epochs.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): The dataset to train the model on.\n",
        "            epochs (int): Number of training epochs.\n",
        "        \"\"\"\n",
        "        loss_function = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "        # preparing instances for training\n",
        "        instances = []\n",
        "        for sentence in dataset:\n",
        "            for i in range(1, len(sentence)):\n",
        "                current_word, current_tag = sentence[i]\n",
        "                previous_word, previous_tag = sentence[i - 1]\n",
        "                prev_tag_one_hot = th.zeros(self.tagset_size, dtype=th.float32).to(self.device)\n",
        "                prev_tag_one_hot[tags_vocab[previous_tag]] = 1\n",
        "                instances.append((words_vocab[current_word], words_vocab[previous_word], prev_tag_one_hot, tags_vocab[current_tag]))\n",
        "\n",
        "        loss_c = 0\n",
        "        batch_size = 32\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(instances)  # Shuffle training data each epoch\n",
        "            for index in range(0, len(instances), batch_size):\n",
        "                batch = instances[index:index + batch_size]\n",
        "                if len(batch) == 0: continue\n",
        "                self.zero_grad()\n",
        "\n",
        "                batch_losses = []\n",
        "\n",
        "                for current_word, previous_word, prev_tag_one_hot, target_tag in batch:\n",
        "                    target_tag_tensor = th.tensor([target_tag], dtype=th.long, device=self.device)\n",
        "                    tag_scores = self(current_word, previous_word, prev_tag_one_hot)\n",
        "\n",
        "                    loss = loss_function(tag_scores, target_tag_tensor)\n",
        "                    batch_losses.append(loss)\n",
        "\n",
        "                # Calculate the average loss for the batch\n",
        "                batch_loss = th.stack(batch_losses).mean()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_c += batch_loss.item()\n",
        "\n",
        "            scheduler.step()\n",
        "            print(f\"Epoch: {epoch+1}, Loss: {loss_c / len(instances)}\")\n",
        "            loss_c = 0\n",
        "\n",
        "\n",
        "    def inference(self, sentence: list) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Predicts the tags for each word in a given sentence.\n",
        "\n",
        "        Args:\n",
        "            sentence (list): The sentence to tag, given as a list of words.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of tuples containing each word and its predicted tag.\n",
        "        \"\"\"\n",
        "\n",
        "        # TO DO ----------------------------------------------------------------------\n",
        "        predicted_tags = []\n",
        "        prev_tag = \"<<UNK>>\"\n",
        "        for word in sentence:\n",
        "            current_word_index = words_vocab.get(word, 0)\n",
        "            previous_word_index = words_vocab.get(prev_tag, 0)\n",
        "            prev_tag_one_hot = th.zeros(self.tagset_size, dtype=th.float32).to(self.device)\n",
        "            prev_tag_one_hot[tags_vocab[prev_tag]] = 1\n",
        "            tag_scores = self(current_word_index, previous_word_index, prev_tag_one_hot)\n",
        "            predicted_tag_index = th.argmax(tag_scores, dim=1).item()\n",
        "            predicted_tag = reversed_tags_vocab[predicted_tag_index]\n",
        "            predicted_tags.append((word, predicted_tag))\n",
        "            prev_tag = predicted_tag\n",
        "\n",
        "        return predicted_tags\n",
        "        # TO DO ----------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_IoQJ2PXOaZ"
      },
      "outputs": [],
      "source": [
        "# Initialize the model with pre-trained embeddings\n",
        "device = th.device(\"cuda\")\n",
        "embeddings_tagger = EmbeddingsTagger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IDlGSTbd-CZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2d1929-5ee7-4ce0-9c2a-7bd028d40ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.014753328329432413\n",
            "Epoch: 2, Loss: 0.00895688952089921\n",
            "Epoch: 3, Loss: 0.008069197575755688\n",
            "Epoch: 4, Loss: 0.006610319915731964\n",
            "Epoch: 5, Loss: 0.006346623375273717\n",
            "Epoch: 6, Loss: 0.006165596153942433\n",
            "Epoch: 7, Loss: 0.005967680582533525\n",
            "Epoch: 8, Loss: 0.005982114258448656\n",
            "Epoch: 9, Loss: 0.005943892763503171\n",
            "Epoch: 10, Loss: 0.005883101668112492\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.80      0.82      0.81      1622\n",
            "         ADP       0.90      0.84      0.87      2481\n",
            "         ADV       0.78      0.73      0.75      1114\n",
            "         AUX       0.88      0.94      0.91      1189\n",
            "       CCONJ       0.90      0.95      0.92       839\n",
            "         DET       0.98      0.90      0.94      2111\n",
            "        INTJ       0.67      0.62      0.64       163\n",
            "        NOUN       0.86      0.93      0.89      4239\n",
            "         NUM       0.63      0.82      0.71       440\n",
            "        PART       0.70      0.79      0.74       519\n",
            "        PRON       0.92      0.92      0.92      1746\n",
            "       PROPN       0.84      0.83      0.83      1628\n",
            "       PUNCT       0.99      0.99      0.99      3027\n",
            "       SCONJ       0.40      0.54      0.46       340\n",
            "         SYM       0.53      0.57      0.55        35\n",
            "        VERB       0.93      0.78      0.85      2480\n",
            "           X       0.40      0.19      0.26        32\n",
            "\n",
            "    accuracy                           0.87     24005\n",
            "   macro avg       0.77      0.77      0.77     24005\n",
            "weighted avg       0.88      0.87      0.87     24005\n",
            "\n"
          ]
        }
      ],
      "source": [
        "precision_embeddings, recall_embeddings, f1_embeddings = train_evaluate_tagger(embeddings_tagger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZO0uGL-4S-"
      },
      "source": [
        "\n",
        "# Part 4 - NLTK Tagger\n",
        "\n",
        "### Overview\n",
        "In this final part of the assignment, you will evaluate the performance of the HMM-based and feed-forward taggers you developed against a Maximum Entropy Markov Model (MEMM) tagger implemented using the Natural Language Toolkit (NLTK), a popular NLP library. Perform comparison should cover the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl3oahPVpsBt"
      },
      "source": [
        "#### Step 1: Training the MEMM Tagger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmkNPYzOvDdf"
      },
      "outputs": [],
      "source": [
        " # TO DO ----------------------------------------------------------------------\n",
        "tnt_tagger = tnt.TnT()\n",
        "tnt_tagger.train(train_dataset)\n",
        " # TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ8YwpiyvJ6l"
      },
      "source": [
        "#### Step 2: Evaluation\n",
        "- Evaluate the trained MEMM tagger on  the test dataset.\n",
        "- Calculate performance metrics such as accuracy, and F1-score. NLTK provides utilities that can help compute these metrics efficiently.\n",
        "- Save & Print the eveluation scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnw1UbMj1n3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f45aed-786b-46c9-c542-6822ee158826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8636534055405124"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        " # TO DO ----------------------------------------------------------------------\n",
        " # Accurcy\n",
        "accuracy_tnt_pos_tagger = tnt_tagger.accuracy(test_dataset)\n",
        " # TO DO ----------------------------------------------------------------------\n",
        "accuracy_tnt_pos_tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-7FX8tW0y7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c11282-2cff-4905-a74a-3ed9df32dba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.90      0.78      0.84      1622\n",
            "         ADP       0.91      0.90      0.91      2481\n",
            "         ADV       0.90      0.76      0.83      1114\n",
            "         AUX       0.88      0.94      0.91      1189\n",
            "       CCONJ       0.99      1.00      0.99       839\n",
            "         DET       0.98      0.97      0.97      2111\n",
            "        INTJ       0.76      0.82      0.79       163\n",
            "        NOUN       0.94      0.76      0.84      4239\n",
            "         NUM       0.96      0.82      0.88       440\n",
            "        PART       0.68      0.89      0.77       519\n",
            "        PRON       0.91      0.98      0.94      1746\n",
            "       PROPN       0.94      0.51      0.66      1628\n",
            "       PUNCT       0.99      1.00      1.00      3027\n",
            "       SCONJ       0.86      0.36      0.51       340\n",
            "         SYM       0.82      0.51      0.63        35\n",
            "         Unk       0.00      0.00      0.00         0\n",
            "        VERB       0.89      0.75      0.82      2480\n",
            "           X       0.83      0.47      0.60        32\n",
            "\n",
            "    accuracy                           0.84     24005\n",
            "   macro avg       0.84      0.74      0.77     24005\n",
            "weighted avg       0.93      0.84      0.87     24005\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        " # TO DO ----------------------------------------------------------------------\n",
        " # F-measure for each tag\n",
        "pred_tags = [tag for sentence in test_dataset for word, _ in sentence for _, tag in tnt_tagger.tag([word])]\n",
        "true_tags = [tag for sent in test_dataset for _, tag in sent]\n",
        "report = precision_recall_fscore_support(true_tags, pred_tags, average=None)\n",
        "report = classification_report(true_tags, pred_tags)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "\n",
        " # TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG4PmdJ_qRHi"
      },
      "source": [
        "# Evaluatoin and Comparison\n",
        "Compare the results obtained from the MEMM tagger with those from your HMM and feed-forward neural network taggers.\n",
        "\n",
        "This part won't be tested by the autograder.\n",
        "\n",
        "#### Discuss the following:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXpQoDsdqihY"
      },
      "source": [
        "\n",
        "* <font color='red'>(**?**)</font> Which tagger performed best on each dataset and why?\n",
        "* <font color='green'>(**Answer**)</font> :\n",
        "\n",
        "The pre-trained embeddings model performed best overall due to its highest accuracy (0.87) and recall (0.77), as well as a strong F1-score (0.77, tied with the NLTK tagger). This model's use of pre-trained word embeddings allows it to capture complex patterns and contextual nuances in the data, leading to superior performance across various metrics. The high recall indicates that it effectively identifies most relevant instances, contributing to its overall effectiveness in POS tagging.\n",
        "\n",
        "The NLTK tagger (MEMM) also performed very well, with the highest precision (0.84) and a strong F1-score (0.77). This high precision suggests that the MEMM tagger is very good at correctly identifying true positives, which is likely due to its flexibility in incorporating a variety of contextual features. However, its slightly lower recall (0.74) compared to the pre-trained embeddings model means it missed a few more relevant instances, placing it just behind in overall performance.\n",
        "\n",
        "The HMM, while simpler and faster to train, lagged behind in recall (0.68) and F1-score (0.70). This lower recall indicates that the HMM missed a significant number of relevant instances, reducing its overall effectiveness. Its inability to capture long-range dependencies and handle rare or unknown words as effectively as the other models explains its lower performance on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SgZs91PqyIG"
      },
      "source": [
        "* <font color='red'>(**?**)</font> What are the strengths and weaknesses of each approach (HMM, feed-forward neural network, MEMM) in terms of training time, accuracy, and generalizability?\n",
        "* <font color='green'>(**Answer**)</font> :\n",
        "\n",
        "Hidden Markov Models (HMMs) are simple and quick to train, making them easy to implement for part-of-speech tagging. They work by using probabilities to predict sequences, which is straightforward and interpretable. However, HMMs often struggle with accuracy and can't handle long-range dependencies well. They also have difficulty with rare and unknown words, limiting their effectiveness in complex language tasks.\n",
        "\n",
        "Feed-forward neural networks with pre-trained word embeddings achieve high accuracy and can generalize well across different contexts. They use embeddings to understand the nuances of language, making them powerful for tasks like POS tagging. However, these models are resource-intensive and take a long time to train. Additionally, they are complex and function like black boxes, making them hard to interpret and understand.\n",
        "\n",
        "Maximum Entropy Markov Models (MEMMs) provide a good balance between flexibility and performance. They use a variety of features to improve accuracy, making them adaptable to different contexts. However, creating these features can be time-consuming and requires expertise. MEMMs also have a moderate training time and can be biased towards certain states due to the label bias problem. Despite these issues, MEMMs are effective for POS tagging, offering strong performance and adaptability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9_TVEPUKkO"
      },
      "source": [
        "# Testing\n",
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJgUy9qNUE0l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "897a56f8-36be-4013-8c39-68ce0315ec49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a9c62f16-fb60-47c0-8a9b-2d2416808fa2\", \"results.json\", 526)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "####################\n",
        "# PLACE TESTS HERE #\n",
        "def test_read_data_data_types():\n",
        "    data = read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")\n",
        "    result = {\n",
        "        'is_data_list': type(data) == list,\n",
        "        'is_data_first_element_list': type(data[0]) == list,\n",
        "        'is_data_first_element_first_item_tuple': type(data[0][0]) == tuple\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test_read_data_len_train_data():\n",
        "    return {\n",
        "        'train_data_length': len(read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")),\n",
        "    }\n",
        "\n",
        "def test_generate_vocabs():\n",
        "    return {\n",
        "        'vocab_size': len(words_vocab),\n",
        "        'num_tags': len(tags_vocab)\n",
        "    }\n",
        "\n",
        "def test_hmm():\n",
        "    return {\n",
        "        'precision': round(precision_hmm, 2),\n",
        "        'recall': round(recall_hmm, 2),\n",
        "        'f1': round(f1_hmm, 2),\n",
        "    }\n",
        "\n",
        "def test_embeddings_model():\n",
        "    return {\n",
        "        'precision': round(precision_embeddings, 2),\n",
        "        'recall': round(recall_embeddings, 2),\n",
        "        'f1': round(f1_embeddings, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "def test_nltk_tagger():\n",
        "    return {\n",
        "        'accuracy': round(accuracy_tnt_pos_tagger, 2)\n",
        "    }\n",
        "\n",
        "TESTS = [test_read_data_data_types, test_read_data_len_train_data, test_generate_vocabs, test_hmm, test_embeddings_model, test_nltk_tagger]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "files.download('results.json')\n",
        "\n",
        "####################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDQsVMnHh1nr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}